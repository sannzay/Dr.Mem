# Dr. Mem ‚Äî AI-Powered Clinical Companion

<p align="center">
<img src="DrMemCompanionApp/Assets.xcassets/AppIcon.appiconset/icon.png" width="120" alt="Dr. Mem App Icon" />
</p>

<p align="center">
<strong>Capture. Organize. Remember. Everything that matters in clinical practice.</strong>
</p>

<p align="center">
<img src="https://img.shields.io/badge/iOS-18.0+-black?style=flat-square&logo=apple" />
<img src="https://img.shields.io/badge/Swift-6.0-orange?style=flat-square&logo=swift" />
<img src="https://img.shields.io/badge/SwiftUI-5-blue?style=flat-square" />
<img src="https://img.shields.io/badge/SwiftData-Enabled-green?style=flat-square" />
<img src="https://img.shields.io/badge/License-MIT-lightgrey?style=flat-square" />
</p>

---

## Overview

**Dr. Mem** is a native iOS companion app built for clinicians and medical students. It captures conversations, lectures, and patient visits ‚Äî then uses AI to extract structured notes, clinical memories, and action items automatically.

Think of it as a second brain that lives in your pocket: always listening (when you want it to), always organizing, always ready to answer questions grounded in what you've actually seen and heard.

> Built with SwiftUI, SwiftData, OpenRouter LLM integration, Omi BLE wearable support, and Apple on-device Speech Recognition.

---

## Features

### üéôÔ∏è Capture Modes

| Mode | Description |
|------|-------------|
| **Education** | Teaching rounds, supervisor feedback, clinical pearls |
| **Brain Dump** | Personal productivity, quick notes ‚Üí tasks |
| **Patient Encounter** | Structured clinical documentation with patient safety guardrails |

### ü§ñ AI Pipeline

- **Session Summarization** ‚Äî Auto-summarize any recording
- **Memory Extraction** ‚Äî Pull clinical pearls, decisions, and plans into a searchable memory bank
- **Task Extraction** ‚Äî Detect action items from conversation and auto-create tasks
- **Clinician Draft Generation** ‚Äî SOAP / H&P structured note drafts
- **Patient AVS Generation** ‚Äî Plain-language After Visit Summaries for patients
- **RAG-Grounded Chat** ‚Äî Ask questions; get answers grounded in your own memories with citations

### üìã Encounters

A first-class module for patient visits:
- Consent gate with audit trail before any recording starts
- Clinician-facing structured note (SOAP/H&P)
- Patient-facing After Visit Summary (AVS)
- Exportable as plain text or PDF
- Default `noteOnly` retention ‚Äî audio deleted after transcription

### üß† Memories

- Searchable, filterable memory bank sourced from all sessions and journal entries
- Types: Learning Pearl, Feedback, Decision, Plan, Task Candidate, Reference
- Pin important memories; link directly to Encounters
- Deep-link back to source session or journal entry

### ‚úÖ Tasks

- Auto-extracted from recordings and journal entries
- Sections: Today / Upcoming / Done
- Local notifications for due dates
- Filter by encounter, priority, or status

### üìì Journal

- Text, voice, and image entries
- Auto memory + task extraction on save
- Clean, Notes-like minimal UI

### üí¨ AI Chat (Dr. Mem identity)

- Claude-like minimal chat interface powered by your chosen model
- Retrieval-Augmented Generation (RAG) over your own memories
- Citation chips showing which memories grounded each answer
- Quick actions: Save as Memory, Make Task, Add to Journal

### üì° Omi Wearable Integration

- Connects to [Omi](https://www.omi.me) BLE wearable via CoreBluetooth
- Live transcription streamed directly from device audio
- Automatic reconnect on disconnect
- Supports Omi, Friend, and Based hardware namespaces

---

## Architecture

```
DrMemCompanionApp/
‚îú‚îÄ‚îÄ Models/
‚îÇ   ‚îú‚îÄ‚îÄ Session.swift              # Recording sessions + encounter metadata
‚îÇ   ‚îú‚îÄ‚îÄ Memory.swift               # Extracted memories / clinical pearls
‚îÇ   ‚îú‚îÄ‚îÄ TaskItem.swift             # Action items
‚îÇ   ‚îú‚îÄ‚îÄ JournalEntry.swift         # Journal entries (text / voice / image)
‚îÇ   ‚îú‚îÄ‚îÄ ChatThread.swift           # Chat conversation threads
‚îÇ   ‚îú‚îÄ‚îÄ TranscriptSegment.swift    # Per-segment transcript data
‚îÇ   ‚îî‚îÄ‚îÄ AppEnums.swift             # All app-wide enumerations
‚îÇ
‚îú‚îÄ‚îÄ Services/
‚îÇ   ‚îú‚îÄ‚îÄ OpenRouterService.swift    # LLM calls + SSE streaming
‚îÇ   ‚îú‚îÄ‚îÄ AIPipelineService.swift    # Summarization, extraction, generation
‚îÇ   ‚îú‚îÄ‚îÄ SpeechRecognitionService.swift  # Apple on-device STT
‚îÇ   ‚îú‚îÄ‚îÄ OmiBLEService.swift        # Omi wearable CoreBluetooth integration
‚îÇ   ‚îú‚îÄ‚îÄ RAGService.swift           # Local keyword + recency retrieval
‚îÇ   ‚îú‚îÄ‚îÄ BiometricService.swift     # Face ID / Touch ID app lock
‚îÇ   ‚îî‚îÄ‚îÄ KeychainService.swift      # Secure API key storage
‚îÇ
‚îú‚îÄ‚îÄ ViewModels/
‚îÇ   ‚îú‚îÄ‚îÄ SessionViewModel.swift
‚îÇ   ‚îî‚îÄ‚îÄ ChatViewModel.swift
‚îÇ
‚îú‚îÄ‚îÄ Views/
‚îÇ   ‚îú‚îÄ‚îÄ ContentView.swift          # Root navigation + drawer
‚îÇ   ‚îú‚îÄ‚îÄ DrawerView.swift           # Slide-out navigation drawer
‚îÇ   ‚îú‚îÄ‚îÄ ListeningView.swift        # Omi + mic recording UI
‚îÇ   ‚îú‚îÄ‚îÄ EncountersView.swift       # Encounter timeline
‚îÇ   ‚îú‚îÄ‚îÄ EncounterDetailView.swift  # Tabbed encounter detail
‚îÇ   ‚îú‚îÄ‚îÄ MemoriesView.swift
‚îÇ   ‚îú‚îÄ‚îÄ TasksView.swift
‚îÇ   ‚îú‚îÄ‚îÄ JournalView.swift
‚îÇ   ‚îú‚îÄ‚îÄ ChatView.swift
‚îÇ   ‚îú‚îÄ‚îÄ ChatsListView.swift
‚îÇ   ‚îú‚îÄ‚îÄ ConsentGateView.swift      # Patient consent workflow
‚îÇ   ‚îú‚îÄ‚îÄ ModePickerSheet.swift
‚îÇ   ‚îú‚îÄ‚îÄ LockScreenView.swift
‚îÇ   ‚îî‚îÄ‚îÄ SettingsView.swift
‚îÇ
‚îî‚îÄ‚îÄ Utilities/
  ‚îú‚îÄ‚îÄ GlassComponents.swift      # Reusable liquid glass UI components
  ‚îî‚îÄ‚îÄ Theme.swift                # Colors, typography, spacing tokens
```

**Pattern:** MVVM with `@Observable` view models, SwiftData for on-device persistence, and a service layer for all external integrations.

---

## Tech Stack

| Layer | Technology |
|-------|-----------|
| UI Framework | SwiftUI 5 |
| Minimum Target | iOS 18.0 |
| Language | Swift 6 (strict concurrency) |
| State Management | `@Observable`, `@State`, `@Binding` |
| Persistence | SwiftData (`@Model`, `@Query`) |
| LLM Gateway | [OpenRouter](https://openrouter.ai) (streaming SSE) |
| Speech-to-Text | Apple `SFSpeechRecognizer` (on-device) |
| BLE Wearable | CoreBluetooth ‚Äî Omi device protocol |
| Security | Keychain (API keys), LocalAuthentication (biometrics) |
| Notifications | `UserNotifications` framework |

---

## Getting Started

### Prerequisites

- Xcode 16 or later
- iOS 18.0+ device *(recommended)* or simulator
- [OpenRouter](https://openrouter.ai) API key ‚Äî free tier available
- *(Optional)* Omi wearable device for BLE audio capture

### Installation

```bash
git clone https://github.com/your-username/dr-mem.git
cd dr-mem
open DrMemCompanionApp.xcodeproj
```

Build and run on your device or simulator. All Swift Package Manager dependencies resolve automatically on first build.

### Configuration

1. Launch the app and open the **drawer** (hamburger icon, top-left)
2. Go to **Settings**
3. Enter your **OpenRouter API Key** and tap **Save Key**
4. Select your preferred **LLM model** (default: `anthropic/claude-sonnet-4`)
5. *(Optional)* Enable **App Lock** for Face ID / Touch ID protection

> API keys are stored exclusively in the iOS Keychain and never written to disk.

---

## Omi Wearable Setup

1. Power on your Omi device (red light = awaiting connection)
2. Navigate to **Listening** from the drawer
3. Tap **Scan for Device** ‚Äî the app discovers and connects automatically
4. Once connected, tap **Start Session** and choose a capture mode
5. Live transcription appears in real time via Apple on-device speech recognition

> Bluetooth Low Energy requires a **physical iPhone**. BLE is not available on the iOS Simulator.

---

## Patient Encounter Workflow

Dr. Mem enforces a structured, privacy-first workflow for patient visits:

```
1. Select "Patient Encounter" mode
       ‚Üì
2. Consent Gate
 ‚Ä¢ Script for clinician to read to patient
 ‚Ä¢ Clinician checkbox attestation (required)
 ‚Ä¢ Optional: patient alias + visit type
 ‚Ä¢ Timestamp recorded automatically (audit trail)
       ‚Üì
3. Recording
 ‚Ä¢ Prominent "Recording ON" banner
 ‚Ä¢ Live rolling transcript
       ‚Üì
4. Review & Approve  ‚Üê required before any export
 ‚Ä¢ Editable Clinician Draft (SOAP/H&P)
 ‚Ä¢ Editable Patient AVS (plain language)
 ‚Ä¢ Redaction suggestion tools
 ‚Ä¢ "Reviewed for accuracy" checkbox
       ‚Üì
5. Store + Export
 ‚Ä¢ Clinician Draft ‚Üí Copy to clipboard / Share sheet (EHR paste)
 ‚Ä¢ Patient AVS ‚Üí PDF generation + Share sheet
 ‚Ä¢ Audio deleted by default (noteOnly retention policy)
```

---

## Privacy & Security

| Feature | Default |
|---------|---------|
| Audio retention after transcription | **Deleted (OFF)** |
| Patient encounter transcript storage | **Note Only** |
| Clinician review required before export | **ON** |
| API key storage | **Keychain only** |
| App Lock (Face ID / Touch ID) | Configurable in Settings |
| Speech recognition | **On-device (Apple STT)** |

All patient data stays **on-device**. The only external network call is the LLM prompt sent to OpenRouter for AI features ‚Äî which you configure and control.

---

## Supported AI Models

Dr. Mem uses [OpenRouter](https://openrouter.ai) as a unified LLM gateway:

| Model | ID |
|-------|----|
| Claude Sonnet 4 *(default)* | `anthropic/claude-sonnet-4` |
| Claude 3 Haiku | `anthropic/claude-3-haiku` |
| GPT-4o | `openai/gpt-4o` |
| GPT-4o Mini | `openai/gpt-4o-mini` |
| Gemini Pro | `google/gemini-pro` |

Switch models anytime in **Settings ‚Üí Model**. Any model available on OpenRouter can be used.

---

## Design System

The UI is built on a **Liquid Glass** design language ‚Äî blurred translucent surfaces, warm gradients, and subtle specular highlights ‚Äî following Apple Human Interface Guidelines.

### Core Components

| Component | Description |
|-----------|-------------|
| `GlassCard` | `.ultraThinMaterial` blur + thin border + gradient highlight |
| `GlassButton` | Pill-shaped frosted action buttons with haptic feedback |
| `GlassInputBar` | Frosted glass input bar for chat and search |
| `GlassSheet` | Modal bottom sheet with glass background |

### Visual Language

- **Background:** Warm ivory / off-white
- **Accent:** Warm terracotta / brown
- **Surfaces:** `.ultraThinMaterial` + soft inner glow
- **Typography:** SF Pro with editorial weight hierarchy
- **Motion:** Spring animations, sensory haptic feedback on key actions

---

## Roadmap

- [ ] iCloud sync across devices
- [ ] HealthKit integration (vitals tagging in encounters)
- [ ] PDF branding for Patient AVS export
- [ ] Home screen widget ‚Äî Today's tasks + recent memory
- [ ] Siri Shortcuts for quick voice capture
- [ ] FHIR export for EHR integration
- [ ] Semantic search with local embeddings (RAG v2)
- [ ] watchOS companion for quick memory review

---

## Contributing

Contributions are welcome. Please open an issue first to discuss your idea.

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/your-feature`
3. Commit your changes: `git commit -m 'Add your feature'`
4. Push to the branch: `git push origin feature/your-feature`
5. Open a Pull Request

---

## License

This project is licensed under the **MIT License**. See [`LICENSE`](LICENSE) for details.

---

## Acknowledgements

- [Omi / Based Hardware](https://www.omi.me) ‚Äî open wearable BLE protocol
- [OpenRouter](https://openrouter.ai) ‚Äî unified LLM API gateway
- Apple ‚Äî SwiftUI, SwiftData, SFSpeechRecognizer, LocalAuthentication

---

<p align="center">
Built with ‚ù§Ô∏è for clinicians who deserve better tools.
</p>
